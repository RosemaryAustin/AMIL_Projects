{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rosemary Austin - T04-06 [00] Using Pre-Trained Models Project [Colab]",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "copyright"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RosemaryAustin/AMIL_Projects/blob/master/Rosemary_Austin_T04_06_%5B00%5D_Using_Pre_Trained_Models_Project_%5BColab%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2019 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "khlO4Bu21oZ4",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AlzIlBsScJJ_"
      },
      "source": [
        "# Using Pre-Trained Models Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nTirVS4FWaPx"
      },
      "source": [
        "In this project we will import a pre-existing model that recognizes objects and use the model to identify those objects in a video. We'll edit the video to draw boxes around the identified object and then reassemble the video so that the boxes are shown around objects in the video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7fz_1lEAXeW2"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lbXXnlHyXiKa"
      },
      "source": [
        "### Learning Objectives\n",
        "\n",
        "* Use OpenCV to process images and video.\n",
        "* Use a pre-trained model to identify and label objects in each frame of a video.\n",
        "* Make judgements about classification quality and when to apply predicted labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M62dLnxpX6Le"
      },
      "source": [
        "### Prerequisites\n",
        "\n",
        "* Classification\n",
        "* Saving and Loading Models\n",
        "* OpenCV\n",
        "* Video Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IrPi5LSZYIrw"
      },
      "source": [
        "### Estimated Duration\n",
        "\n",
        "330 minutes (285 minutes working time, 45 minutes for presentations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6RA-x2G2Fd4"
      },
      "source": [
        "### Deliverables\n",
        "\n",
        "1. A copy of this Colab notebook containing your code and responses to the ethical considerations below. The code should produce a functional labeled video.\n",
        "1. A group presentation. After everyone is done, we will ask each group to stand in front of the class and give a brief presentation about what they have done in this lab. The presentation can be a code walkthrough, a group discussion, a slide show, or any other means that conveys what you did over the course of the day and what you learned. If you do create any artifacts for your presentation, please share them in the class folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pzCWjL4DYLgf"
      },
      "source": [
        "### Grading Criteria\n",
        "\n",
        "This project is graded in separate sections that each contribute a percentage of the total score:\n",
        "\n",
        "1. Building and Using a Model (80%)\n",
        "1. Ethical Implications (10%)\n",
        "1. Project Presentation (10%)\n",
        "\n",
        "#### Building and Using a Model\n",
        "\n",
        "There are 6 demonstrations of competency listed in the problem statement below. Each competency is graded on a 3 point scale for a total of 18 available points. The following rubric will be used:\n",
        "\n",
        "| Points | Description |\n",
        "|--------|-------------|\n",
        "| 0      | No attempt at the competency |\n",
        "| 1      | Attempted competency, but in an incorrect manner |\n",
        "| 2      | Attempted competency correctly, but sub-optimally |\n",
        "| 3      | Successful demonstration of competency |\n",
        "\n",
        "\n",
        "#### Ethical Implications\n",
        "\n",
        "There are six questions in the **Ethical Implications** secion. Each question is worth 2 points. The rubric for calculating those points is:\n",
        "\n",
        "| Points | Description |\n",
        "|--------|-------------|\n",
        "| 0      | No attempt at question or answer was off-topic or didn't make sense |\n",
        "| 1      | Question was answered, but answer missed important considerations  |\n",
        "| 2      | Answer adequately considered ethical implications |\n",
        "\n",
        "#### Project Presentation\n",
        "\n",
        "The project presentation will be graded on participation. All members of a team should actively participate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_yACB56w2JVk"
      },
      "source": [
        "## Team\n",
        "\n",
        "Please enter your team members names in the placeholders in this text area:\n",
        "\n",
        "*   Prashamsa \n",
        "*   Christine\n",
        "*   Rosemary\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YTVUYxPwcHhp"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LdIOgOHP1ces"
      },
      "source": [
        "## Exercise 1: Coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jhTEOK1ZmqN8"
      },
      "source": [
        "For this workshop you will process a video frame-by-frame, identify objects in each frame, and draw a bounding box and label around each object.\n",
        " \n",
        "Use the [SSD MobileNet V1 Coco](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) 'ssd_mobilenet_v1_coco' model. The video that you'll be processing can be [found on Pixabay](https://pixabay.com/videos/cars-motorway-speed-motion-traffic-1900/). The 640x360 version of the video is smallest and easiest to handle, though any should work since you must scale down the images for processing.\n",
        " \n",
        "**Graded** demonstrations of competency:\n",
        "1. Obtain the pre-trained model from the [TensorFlow Zoo](https://github.com/tensorflow/models).\n",
        "1. Load the pre-trained model into a TensorFlow object.\n",
        "1. Obtain a video file from Pixabay to use for classification.\n",
        "1. Process the video frame-by-frame creating a modified output video.\n",
        "1. Apply a classification model to an image.\n",
        "1. Draw a bounding box around classified objects in each image.\n",
        " \n",
        "The flow of the program is roughly:\n",
        " \n",
        "* Read in a video file (use the one in this colab if you want)\n",
        "* Load the TensorFlow model\n",
        "* Loop over each frame of the video\n",
        "> * Scale the frame down to a size the model expects\n",
        " * Feed the frame to the model\n",
        " * Loop over detections made by the model\n",
        " >  * If the detection score is above some threshold draw a bounding box onto the frame and put a label in or near the box\n",
        "   * Write the frame back to a new video\n",
        " \n",
        "Some tips:\n",
        " \n",
        "* Processing an entire video is slow, consider truncating the video or skipping over frames during development. Skipping frames will make the video choppy, but you'll be able to see a wider variety of images than you would in a truncated video with all of the original frames in the clip.\n",
        "* The model expects a 300x300 image. You'll likely have to scale your frames to fit the model. When you get a bounding box that box is relative to the scaled image. You'll need to scale the bounding box out to the original image size.\n",
        "* Don't start by trying to process the video. Instead, capture one frame and work with it until you are happy with your object detection, bounding boxes, and labels. Once you get those done worry about video handling.\n",
        "* The [Coco labels file](https://github.com/nightrome/cocostuff/blob/master/labels.txt) can be used to identify classified objects.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7XM35vYWSbim"
      },
      "source": [
        "### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ivTzfzQN5jDk",
        "colab": {}
      },
      "source": [
        "#This is loading in the video file\n",
        "\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "cars_video = cv.VideoCapture('Cars.mp4')\n",
        "\n",
        "height = int(cars_video.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
        "width = int(cars_video.get(cv.CAP_PROP_FRAME_WIDTH))\n",
        "fps = cars_video.get(cv.CAP_PROP_FPS)\n",
        "total_frames = int(cars_video.get(cv.CAP_PROP_FRAME_COUNT))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdcgnPaVRk3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This is uploading the model \n",
        "\n",
        "import urllib.request\n",
        "import os\n",
        "\n",
        "base_url = 'http://download.tensorflow.org/models/object_detection/'\n",
        "file_name = 'ssd_mobilenet_v1_coco_2018_01_28.tar.gz'\n",
        "\n",
        "url = base_url + file_name\n",
        "\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "os.listdir()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxPoErkbgXkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract the Model Data\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "dir_name = file_name[0:-len('.tar.gz')]\n",
        "\n",
        "if os.path.exists(dir_name):\n",
        "  shutil.rmtree(dir_name) \n",
        "\n",
        "tarfile.open(file_name, 'r:gz').extractall('./')\n",
        "\n",
        "os.listdir(dir_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSW3HDZghZH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The detections that the model returns\n",
        "outputs = (\n",
        "    'num_detections',\n",
        "    'detection_classes',\n",
        "    'detection_scores',\n",
        "    'detection_boxes',\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdL4ffD3g9c8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QFuWUoRf4w6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating a dictionary with just id and lables \n",
        "#receiving the id and the display_name from the data\n",
        "with open('mscoco_complete_label_map.pbtxt.txt', 'r') as file:\n",
        "    data = file.read().replace('\\n', '')\n",
        "labels = {}\n",
        "for val in data.split('{')[1:]:\n",
        "  label = val.split('display_name: ')[1].split('}')[0]\n",
        "  ident = val.split('id: ')[1].split(' ')[0]\n",
        "  labels[ident] = label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqoLKE24i6EN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhGn8emLidJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# looking at the detected lables and storing that in a list\n",
        "def label_dec (image_f, lable1):\n",
        "  labels_ = []\n",
        "  for var in image_f[1][0]:\n",
        "    if var!=1.0:\n",
        "      labels_.append(labels[str(int(var))[:2]])\n",
        "  return (labels_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY4BZLemgB2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu-CQRX-gCqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# brings the model into a graph\n",
        "\n",
        "import tarfile\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dir_name = file_name[0:-len('.tar.gz')]\n",
        "\n",
        "if os.path.exists(dir_name):\n",
        "  shutil.rmtree(dir_name) \n",
        "\n",
        "tarfile.open(file_name, 'r:gz').extractall('./')\n",
        "\n",
        "os.listdir(dir_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "162uxfyEhR6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This puts the model in a graph so that it can be used in tensorflow\n",
        "frozen_graph = os.path.join(dir_name, 'frozen_inference_graph.pb')\n",
        "\n",
        "with tf.gfile.FastGFile(frozen_graph,'rb') as f:\n",
        "  graph_def = tf.GraphDef()\n",
        "  graph_def.ParseFromString(f.read())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgVDDt-5tUKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This recolors the frame and resizes it in readable form\n",
        "def frame_fix(image):\n",
        "  frame = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
        "  frame = cv.resize(frame, (300, 300))\n",
        "  return frame\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pmere-I97uqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This is bringing in a diffrent label file \n",
        "#as the original one was returning the wrong detections\n",
        "#creating a dictionary with just id and lables \n",
        "#receiving the id and the display_name from the data\n",
        "open_new = open(\"labels (1).txt\", \"r\")\n",
        "label2={}\n",
        "for x in open_new:\n",
        "  key,_,value=x.partition(':')\n",
        "  value,_,_=value.partition('\\n')\n",
        "  label2[key]=value\n",
        "\n",
        "# label1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTGHvcyT70DQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#detections[1][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_PEHVECSJJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This the amount of frames in the Cars.mp4\n",
        "\n",
        "total_frames = int(cars_video.get(cv.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "#This is makeing a place holder for the new labled video to be stored in\n",
        "fourcc = cv.VideoWriter_fourcc(*'mp4v')\n",
        "output_video = cv.VideoWriter('Cars-short.mp4', fourcc, fps, (300, 300))\n",
        "\n",
        "\n",
        "# cycles through the frames of the Cars.mp4 at a ten step\n",
        "for current_frame in range(0, total_frames,10): \n",
        "  cars_video.set(cv.CAP_PROP_POS_FRAMES, current_frame)\n",
        "  ret, frame = cars_video.read()\n",
        "  #if the program crashes shows us where it failed in the loop\n",
        "  if not ret:\n",
        "    raise Exception(f'Problem reading frame {current_frame} from video')\n",
        "  \n",
        "  #This passes the frame to be formated\n",
        "  frame = frame_fix(frame)\n",
        "  image = [frame] #image into a list \n",
        "  \n",
        "  #starts tensor flow sess to create the graph \n",
        "  with tf.Session() as sess:\n",
        "    sess.graph.as_default()\n",
        "    tf.import_graph_def(graph_def, name='')\n",
        "    \n",
        "    #passes it to the model to find objects in frame\n",
        "    detections = sess.run(\n",
        "        [sess.graph.get_tensor_by_name(f'{op}:0') for op in outputs],\n",
        "        feed_dict={ 'image_tensor:0': image })\n",
        "    \n",
        "    # Must be diffrent names other wise it will over ride it ever loop\n",
        "    label1 = label_dec(detections, label2)\n",
        "\n",
        "    \n",
        "    #making data frame to hold the results of model outputs\n",
        "    df1 = pd.DataFrame()\n",
        "    top = detections[3][0][:len(label1)][:,0]\n",
        "    left = detections[3][0][:len(label1)][:,1]\n",
        "    bottom = detections[3][0][:len(label1)][:,2]\n",
        "    right = detections[3][0][:len(label1)][:,3]\n",
        "    scale = 1.0\n",
        "\n",
        "    df1['labels'] = label1\n",
        "    df1['top'] = top\n",
        "    df1['left'] = left\n",
        "    df1['bottom'] = bottom\n",
        "    df1['right'] = right\n",
        "    \n",
        "    #print(df1)\n",
        "    \n",
        "    #This draws the boxes around the objects detected\n",
        "    width,height,colour = frame.shape\n",
        "    for index, row in df1.iterrows():\n",
        "      left = int(row['left']*height)\n",
        "      top = int(row['top']*width)\n",
        "      right = int(row['right']*height)\n",
        "      bottom = int(row['bottom']*width)\n",
        "      #if row['labels'] == '\"cars\"':\n",
        "      frame = cv.rectangle(frame,(left,top),(right,bottom),[0,255,0],2)\n",
        "      frame = cv.putText(frame, df1['labels'][index] , (left,top), cv.FONT_HERSHEY_SIMPLEX,scale,[0,255,0],2)\n",
        "    output_video.write(frame)\n",
        "#     plt.imshow(frame)\n",
        "#     plt.show()\n",
        "  \n",
        "  \n",
        "  \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBfu1kkAFcj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#releases the videos from the RAM\n",
        "cars_video.release()\n",
        "output_video.release()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HniKdSXg0YHR"
      },
      "source": [
        "## Exercise 2: Ethical Implications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W4FvC1Aa0ZT5"
      },
      "source": [
        "Even the most basic of models have the potential to affect segments of the population in different ways. It is important to consider how your model might positively and negative effect different types of users.\n",
        "\n",
        "In this section of the project you will reflect on the positive and negative implications of your model.\n",
        "\n",
        "Frame the context of your models creation using this narriative:\\n\",\n",
        "\n",
        "  > The city of Seattle is attempting to reduce traffic congestion in their downtown area. As part of this project, they plan on allowing each local driver one free downtown trip per week. After that the driver will have to pay a $50 toll for each extra day per week driven. As an early proof-of-concept for this project your team is tasked with using machine learning to correctly identify automobiles on the road. The next phase of the project will involve detecting license plate numbers and then cross-referencing that data with RFID chips that should be mounted in all local drivers cars.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lkyzwVQr0brd"
      },
      "source": [
        "### Student Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gy4I2vG60ebd"
      },
      "source": [
        "**Positive Impact**\n",
        "\n",
        "Your model is trying to solve a problem. Think about who will benefit from that problem being solved and write a brief narrative about how the model will help.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k59MK1Ah0fWy"
      },
      "source": [
        "The local police force has been tasked with discovering the cause for so many accidents on heavly travled road. Thanks to the model they are able to indetify that people and animals are crossing the road. They were then able to put up a human and animal crossing sign cutting down on the number of accidents happening on that road."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gzqkrLnk0hMU"
      },
      "source": [
        "**Negative Impact**\n",
        "\n",
        "Models don't often have universal benefit. Think about who might be negatively impacted by the predictions your model is making. This person or persons might not be directly using the model, but instead might be impacted indirectly.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hefa1JdP0kj3"
      },
      "source": [
        "The local police force has started using the model. However the model is misidentifying objects on the road. Things that should be appearing as cars are appearing as humans and animals instead. The crossing signs they have put up  have not resolved the cause for so many accidents and wasted limited tax money."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Uax2HAzd0mHX"
      },
      "source": [
        "**Bias**\n",
        "\n",
        "Models can be bias for many reasons. The bias can come from the data used to build the model (eg. sampling, data collection methods, available sources) and from the interpretation of the predictions generated by the model.\n",
        "\n",
        "Think of at least two ways that bias might have been introduced to your model and explain both below.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6bJGm-qs0oQV"
      },
      "source": [
        "One source of bias in the model could be the model training data, where the classifcation of the objects are limited.\n",
        "\n",
        "Another source of bias in the model could be the image quality that the cameras are feeding into the model to idetify."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ybb1zAkC0p2e"
      },
      "source": [
        "**Changing the Dataset to Mitigate Bias**\n",
        "\n",
        "Bias datasets are one of the primary ways in which bias is introduced to a machine learning model. Look back at the input data that you fed to your model. Think about how you might change something about the data to reduce bias in your model.\n",
        "\n",
        "What change or changes could you make to your dataset less bias? Consider the data that you have, how and where that data was collected, and what other sources of data might be used to reduce bias.\n",
        "\n",
        "Write a summary of change that could be made to your input data.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UFsnF4_h08DD"
      },
      "source": [
        "Since we are looking at cars, a better dataset that has more detail description about car, car types, models, vechile types, street signs could be used that fits the purpose of the model. \n",
        "\n",
        "The second option will be to use a dataset that can classifiy more objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ChEJbhXA02pW"
      },
      "source": [
        "**Changing the Model to Mitigate Bias**\n",
        "\n",
        "Is there any way to reduce bias by changing the model itself? This could include modifying algorithmic choices, tweaking hyperparameters, etc.\n",
        "\n",
        "Write a brief summary of changes that you could make to help reduce bias in your model.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kEAhgO_U0p8Y"
      },
      "source": [
        "*   Since the model has potential bias of limited classification of model training data, it could be improved to expand the classification of objects in the traning data to mitigate bias..\n",
        "\n",
        "*   Since the model has potential bias of image quality, it could be improved to feed higher quality images to the model to mitigate bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rShB5BQv0wix"
      },
      "source": [
        "**Mitigating Bias Downstream**\n",
        "\n",
        "Models make predictions. Downstream processes make decisions. What processes and/or rules should be in place for people and systems interpreting and acting on the results of your model to reduce the bias? Describe these below.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C__BwBP-00HN"
      },
      "source": [
        "Beside the prediction from the model, the downstream processes could add the rules of verifying other reasons that might relate to the  traffic safety. They can be the assurance of traffic lights and traffic signs in good condition and function well. Also, they can be the estimation of weather impact including wind, rain or snow."
      ]
    }
  ]
}